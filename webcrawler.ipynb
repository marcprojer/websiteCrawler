{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPKB6q3JXSTqF2xULq8VgdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcprojer/websiteCrawler/blob/main/webcrawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zih6xjHFK3uI",
        "outputId": "db540da7-d986-4480-ce64-92b92b62c18c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY1V9ei8MpT_",
        "outputId": "780c8e79-0ad7-44a0-f52a-13dde1f45cc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete Crawl:"
      ],
      "metadata": {
        "id": "kdnRlz4WOIbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import pandas as pd\n",
        "\n",
        "# Function to get the HTML content of a page\n",
        "def get_html(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract all links from the page\n",
        "def extract_links(html, base_url):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    links = []\n",
        "\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        # Convert relative URLs to absolute URLs\n",
        "        full_url = urljoin(base_url, href)\n",
        "        links.append(full_url)\n",
        "\n",
        "    return links\n",
        "\n",
        "# Basic web crawler function with data export to Excel\n",
        "def crawl_website(start_url, max_pages=5, output_file='crawled_data.xlsx'):\n",
        "    to_visit = [start_url]  # List of URLs to visit\n",
        "    visited = set()         # Set of visited URLs\n",
        "    crawled_data = []       # List to store crawled links\n",
        "\n",
        "    while to_visit and len(visited) < max_pages:\n",
        "        url = to_visit.pop(0)  # Get the next URL to visit\n",
        "\n",
        "        if url not in visited:\n",
        "            print(f\"Visiting: {url}\")\n",
        "            html = get_html(url)\n",
        "\n",
        "            if html is None:\n",
        "                continue\n",
        "\n",
        "            visited.add(url)  # Mark the page as visited\n",
        "            crawled_data.append(url)  # Store visited URL\n",
        "\n",
        "            links = extract_links(html, url)  # Extract links from the page\n",
        "            for link in links:\n",
        "                if link not in visited and link not in to_visit:\n",
        "                    to_visit.append(link)  # Add new links to the visit list\n",
        "\n",
        "    # Save the crawled data to an Excel file using pandas\n",
        "    df = pd.DataFrame(crawled_data, columns=['Crawled URLs'])\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "# Start crawling and export to Excel\n",
        "start_url = \"EXAMPLE\"\n",
        "crawl_website(start_url, max_pages=100, output_file='crawled_links.xlsx')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "f-9QiyYcLFbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust depth:"
      ],
      "metadata": {
        "id": "xQV3QfBoOQLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import pandas as pd\n",
        "\n",
        "# Function to get the HTML content of a page\n",
        "def get_html(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract all links from the page\n",
        "def extract_links(html, base_url):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    links = []\n",
        "\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        # Convert relative URLs to absolute URLs\n",
        "        full_url = urljoin(base_url, href)\n",
        "        links.append(full_url)\n",
        "\n",
        "    return links\n",
        "\n",
        "# Basic web crawler function with depth control and data export to Excel\n",
        "def crawl_website(start_url, max_pages=1, max_depth=1, output_file='crawled_data.xlsx'):\n",
        "    to_visit = [(start_url, 0)]  # List of (URL, depth) tuples to visit\n",
        "    visited = set()              # Set of visited URLs\n",
        "    crawled_data = []            # List to store crawled links\n",
        "\n",
        "    while to_visit and len(visited) < max_pages:\n",
        "        url, depth = to_visit.pop(0)  # Get the next URL and its depth\n",
        "\n",
        "        if url not in visited and depth <= max_depth:\n",
        "            print(f\"Visiting: {url} (Depth: {depth})\")\n",
        "            html = get_html(url)\n",
        "\n",
        "            if html is None:\n",
        "                continue\n",
        "\n",
        "            visited.add(url)  # Mark the page as visited\n",
        "            crawled_data.append(url)  # Store visited URL\n",
        "\n",
        "            if depth < max_depth:  # Only extract links if we're below the max depth\n",
        "                links = extract_links(html, url)  # Extract links from the page\n",
        "                for link in links:\n",
        "                    if link not in visited:\n",
        "                        to_visit.append((link, depth + 1))  # Add new links with incremented depth\n",
        "\n",
        "    # Save the crawled data to an Excel file using pandas\n",
        "    df = pd.DataFrame(crawled_data, columns=['Crawled URLs'])\n",
        "    df.to_excel(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "# Start crawling and export to Excel with depth control\n",
        "start_url = \"EXAMPLE\"\n",
        "crawl_website(start_url, max_pages=300, max_depth=1, output_file='crawled_links_with_depth.xlsx')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kYgXndyEOPze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export for first part:"
      ],
      "metadata": {
        "id": "C_R1ZlICRteL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('crawled_links.xlsx')"
      ],
      "metadata": {
        "id": "lDua1SWGNQVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export for second part:"
      ],
      "metadata": {
        "id": "xrkxq9bcRz9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('crawled_links_with_depth.xlsx')"
      ],
      "metadata": {
        "id": "G7zdBemyQd4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}